{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mitiau/PROSTATA/blob/HSE_seminar/PROSTATA_tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f59Ujuujn___"
   },
   "source": [
    "# Install dependecies and download weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apiUcTpNTnlU",
    "outputId": "d7491c10-dae0-4701-844d-1c76d0353a04",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install fair-esm\n",
    "!pip install biopython\n",
    "!pip install gdown==4.5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bsyfz4BrSxMN"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive, files\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "\n",
    "import transformers\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import esm\n",
    "from esm import ProteinBertModel\n",
    "from esm.pretrained import load_model_and_alphabet_hub\n",
    "\n",
    "from Bio import SeqIO\n",
    "from io import StringIO, BytesIO\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://a025generative-modeling-for-design.obs.ru-moscow-1.hc.sbercloud.ru/hse_protein_seminar/ESMForSingleMutationPosConcat\n",
    "!wget https://a025generative-modeling-for-design.obs.ru-moscow-1.hc.sbercloud.ru/hse_protein_seminar/ESMForSingleMutationPosOuter\n",
    "!wget https://a025generative-modeling-for-design.obs.ru-moscow-1.hc.sbercloud.ru/hse_protein_seminar/ESMForSingleMutation_cls\n",
    "!wget https://a025generative-modeling-for-design.obs.ru-moscow-1.hc.sbercloud.ru/hse_protein_seminar/ESMForSingleMutation_pos\n",
    "!wget https://a025generative-modeling-for-design.obs.ru-moscow-1.hc.sbercloud.ru/hse_protein_seminar/ESMForSingleMutation_pos_cat_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/mitiau/PROSTATA.git\n",
    "!git -C PROSTATA checkout HSE_seminar\n",
    "!git -C PROSTATA pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#@title install ESMfold\n",
    "#@markdown install ESMFold, OpenFold and download Params (~2min 30s)\n",
    "\n",
    "import os, time\n",
    "if not os.path.isfile(\"esmfold.model\"):\n",
    "  # download esmfold params\n",
    "  os.system(\"apt-get install aria2 -qq\")\n",
    "  os.system(\"aria2c -q -x 16 https://colabfold.steineggerlab.workers.dev/esm/esmfold.model &\")\n",
    "\n",
    "  # install libs\n",
    "  os.system(\"pip install -q omegaconf pytorch_lightning biopython ml_collections einops py3Dmol\")\n",
    "  os.system(\"pip install -q git+https://github.com/NVIDIA/dllogger.git\")\n",
    "\n",
    "  # install openfold\n",
    "  commit = \"6908936b68ae89f67755240e2f588c09ec31d4c8\"\n",
    "  os.system(f\"pip install -q git+https://github.com/aqlaboratory/openfold.git@{commit}\")\n",
    "\n",
    "  # install esmfold\n",
    "  os.system(f\"pip install -q git+https://github.com/sokrypton/esm.git\")\n",
    "\n",
    "  # wait for Params to finish downloading...\n",
    "  if not os.path.isfile(\"esmfold.model\"):\n",
    "    # backup source!\n",
    "    os.system(\"aria2c -q -x 16 https://files.ipd.uw.edu/pub/esmfold/esmfold.model\")\n",
    "  else:\n",
    "    while os.path.isfile(\"esmfold.model.aria2\"):\n",
    "      time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYLdBMG7UUx3"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "HIDDEN_UNITS_POS_CONTACT = 5\n",
    "class ESMForSingleMutationPosConcat(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.esm2, _ = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "        self.fc1 = nn.Linear(1280 * 2, HIDDEN_UNITS_POS_CONTACT)\n",
    "        self.fc2 = nn.Linear(HIDDEN_UNITS_POS_CONTACT, 1)\n",
    "\n",
    "    def forward(self, token_ids1, token_ids2, pos):\n",
    "        outputs1 = self.esm2.forward(token_ids1, repr_layers=[33])[\n",
    "            'representations'][33]\n",
    "        outputs2 = self.esm2.forward(token_ids2, repr_layers=[33])[\n",
    "            'representations'][33]\n",
    "        outputs1_pos = outputs1[:, pos + 1]\n",
    "        outputs2_pos = outputs2[:, pos + 1]\n",
    "        outputs_pos_concat = torch.cat((outputs1_pos, outputs2_pos), 2)\n",
    "        fc1_outputs = F.relu(self.fc1(outputs_pos_concat))\n",
    "        logits = self.fc2(fc1_outputs)\n",
    "        return logits\n",
    "    \n",
    "HIDDEN_UNITS_POS_OUTER = 5\n",
    "class ESMForSingleMutationPosOuter(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.esm2, _ = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "        self._freeze_esm2_layers()\n",
    "        self.fc1 = nn.Linear(1280 * 1280, HIDDEN_UNITS_POS_OUTER)\n",
    "        self.fc2 = nn.Linear(HIDDEN_UNITS_POS_OUTER, 1)\n",
    "\n",
    "    def _freeze_esm2_layers(self):\n",
    "        total_blocks = 33\n",
    "        initial_layers = 2\n",
    "        layers_per_block = 16\n",
    "        num_freeze_blocks = total_blocks - 3\n",
    "        for _, param in list(self.esm2.named_parameters())[\n",
    "            :initial_layers + layers_per_block * num_freeze_blocks]:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, token_ids1, token_ids2, pos):\n",
    "        outputs1 = self.esm2.forward(token_ids1, repr_layers=[33])[\n",
    "            'representations'][33]\n",
    "        outputs2 = self.esm2.forward(token_ids2, repr_layers=[33])[\n",
    "            'representations'][33]\n",
    "        outputs1_pos = outputs1[:, pos + 1]\n",
    "        outputs2_pos = outputs2[:, pos + 1]\n",
    "        outer_prod = outputs1_pos.unsqueeze(3) @ outputs2_pos.unsqueeze(2)\n",
    "        outer_prod_view = outer_prod.view(outer_prod.shape[0], outer_prod.shape[1], -1)\n",
    "        fc1_outputs = F.relu(self.fc1(outer_prod_view))\n",
    "        logits = self.fc2(fc1_outputs)\n",
    "        return logits\n",
    "    \n",
    "class ESMForSingleMutation_pos(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.esm1v, self.esm1v_alphabet = esm.pretrained.esm2_t33_650M_UR50D()        \n",
    "        self.classifier = nn.Linear(1280, 1)\n",
    "        self.const1 = torch.nn.Parameter(torch.ones((1,1280)))\n",
    "        self.const2 = torch.nn.Parameter(-1 * torch.ones((1,1280)))\n",
    "        \n",
    "\n",
    "    def forward(self, token_ids1, token_ids2, pos):                \n",
    "        outputs1 = self.esm1v.forward(token_ids1, repr_layers=[33])['representations'][33]\n",
    "        outputs2 = self.esm1v.forward(token_ids2, repr_layers=[33])['representations'][33]\n",
    "        outputs = self.const1 * outputs1[:,pos + 1,:] + self.const2 * outputs2[:,pos + 1,:]        \n",
    "        logits = self.classifier(outputs)\n",
    "        return logits\n",
    "    \n",
    "class ESMForSingleMutation_cls(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.esm1v, self.esm1v_alphabet = esm.pretrained.esm2_t33_650M_UR50D()        \n",
    "        self.classifier = nn.Linear(1280, 1)\n",
    "        self.const1 = torch.nn.Parameter(torch.ones((1,1280)))\n",
    "        self.const2 = torch.nn.Parameter(-1 * torch.ones((1,1280)))\n",
    "        \n",
    "\n",
    "    def forward(self, token_ids1, token_ids2, pos):                \n",
    "        outputs1 = self.esm1v.forward(token_ids1, repr_layers=[33])['representations'][33]\n",
    "        outputs2 = self.esm1v.forward(token_ids2, repr_layers=[33])['representations'][33]\n",
    "        outputs = self.const1 * outputs1[:,0,:] + self.const2 * outputs2[:,0,:]        \n",
    "        logits = self.classifier(outputs.unsqueeze(0))\n",
    "        return logits\n",
    "    \n",
    "class ESMForSingleMutation_pos_cat_cls(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.esm1v, self.esm1v_alphabet = esm.pretrained.esm2_t33_650M_UR50D()        \n",
    "        self.classifier = nn.Linear(1280*2, 1)\n",
    "        self.const1 = torch.nn.Parameter(torch.ones((1,1280)))\n",
    "        self.const2 = torch.nn.Parameter(-1 * torch.ones((1,1280)))\n",
    "        \n",
    "\n",
    "    def forward(self, token_ids1, token_ids2, pos):                \n",
    "        outputs1 = self.esm1v.forward(token_ids1, repr_layers=[33])['representations'][33]\n",
    "        outputs2 = self.esm1v.forward(token_ids2, repr_layers=[33])['representations'][33]\n",
    "        cls_out = self.const1 * outputs1[:,0,:] + self.const2 * outputs2[:,0,:]\n",
    "        pos_out = self.const1 * outputs1[:,pos+1,:] + self.const2 * outputs2[:,pos+1,:]\n",
    "        outputs = torch.cat([cls_out.unsqueeze(0), pos_out], axis = -1)        \n",
    "        logits = self.classifier(outputs)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3Pjcm9HvRMC"
   },
   "outputs": [],
   "source": [
    "model_names = ['ESMForSingleMutationPosOuter',\n",
    "          'ESMForSingleMutationPosConcat',\n",
    "          'ESMForSingleMutation_pos_cat_cls',  \n",
    "              'ESMForSingleMutation_pos', \n",
    "              'ESMForSingleMutation_cls']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAp3NQyaupxE"
   },
   "source": [
    "# Compute DeltaDDG for test set and compare with experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('ESMForSingleMutation_cls', map_location=torch.device('cpu'))\n",
    "esm2_alphabet = model.esm1v_alphabet\n",
    "esm2batch_converter = esm2_alphabet.get_batch_converter()\n",
    "\n",
    "def predict_ddg(seqs, mutation_codes, poss = None):\n",
    "    if poss is None:\n",
    "        poss = [None]*len(seqs)\n",
    "    inp = []\n",
    "    for seq, mutation_code, pos in zip(seqs, mutation_codes, poss):\n",
    "        #print(mutation_code)\n",
    "        wt_aa = mutation_code[0]\n",
    "        mut_aa = mutation_code[-1]\n",
    "        if pos:\n",
    "            mut_pos = pos\n",
    "        else:\n",
    "            mut_pos = int(mutation_code[1:-1])-1\n",
    "\n",
    "        assert seq[mut_pos] == wt_aa\n",
    "        \n",
    "        wt = seq\n",
    "        tt = list(seq)\n",
    "        tt[mut_pos] = mut_aa\n",
    "        mut = ''.join(tt)\n",
    "\n",
    "    \n",
    "    \n",
    "        _, _, esm2_batch_tokens1 = esm2batch_converter([('' , wt[:1022])])\n",
    "        _, _, esm2_batch_tokens2 = esm2batch_converter([('' , mut[:1022])])\n",
    "        esm2_batch_tokens1 = esm2_batch_tokens1.cuda()\n",
    "        esm2_batch_tokens2 = esm2_batch_tokens2.cuda()\n",
    "    \n",
    "        inp.append((esm2_batch_tokens1, esm2_batch_tokens2, mut_pos))\n",
    "    \n",
    "    res = []\n",
    "    for model_name in model_names:\n",
    "        model = torch.load(model_name, map_location=torch.device('cpu'))\n",
    "        model.eval()\n",
    "        model.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            res.append([model(token_ids1 = t1, token_ids2 = t2, \n",
    "                             pos = torch.LongTensor([p])).cpu().numpy() for t1, t2, p in inp])\n",
    "        #print(f'Model {model_name} DDG prediction is {res[-1]}')\n",
    "    res = np.mean(res, axis = 0)\n",
    "    return res.ravel()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('PROSTATA/cross_validation_datasets/test_1LNIA.csv')\n",
    "test_df['ddg_pred'] = predict_ddg(test_df['wt_seq'].tolist(), \n",
    "                                  test_df['mut_info'].tolist(), \n",
    "                                  test_df['pos'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = test_df.ddg.to_list()\n",
    "x = test_df.ddg_pred.to_list()\n",
    "plt.scatter(x, y,alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = ['VINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPGKSIGGDIFSNREGKLPGKSGRTWREADINYTSGFRNSDRILYSSDWLIYKTTDHYQTFTKIR']\n",
    "mutation_codes = ['V1N'] #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ddg(seqs, mutation_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAp3NQyaupxE"
   },
   "source": [
    "# Find best mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wildtype = 'VINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPGKSIGGDIFSNREGKLPGKSGRTWREADINYTSGFRNSDRILYSSDWLIYKTTDHYQTFTKIR'\n",
    "wt_aas = list(set(wildtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: изменить на перебор мутаций\n",
    "\n",
    "pos = 1\n",
    "mut_acid = 'A'\n",
    "mutation_code = f'{wildtype[pos -1]}{pos}{mut_acid}'\n",
    "predict_ddg([wildtype], [mutation_code])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAp3NQyaupxE"
   },
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ##predict 3d structure with **ESMFold**\n",
    "%%time\n",
    "from string import ascii_uppercase, ascii_lowercase\n",
    "import hashlib, re, os\n",
    "import numpy as np\n",
    "from jax.tree_util import tree_map\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "\n",
    "def parse_output(output):\n",
    "  pae = (output[\"aligned_confidence_probs\"][0] * np.arange(64)).mean(-1) * 31\n",
    "  plddt = output[\"plddt\"][0,:,1]\n",
    "\n",
    "  bins = np.append(0,np.linspace(2.3125,21.6875,63))\n",
    "  sm_contacts = softmax(output[\"distogram_logits\"],-1)[0]\n",
    "  sm_contacts = sm_contacts[...,bins<8].sum(-1)\n",
    "  xyz = output[\"positions\"][-1,0,:,1]\n",
    "  mask = output[\"atom37_atom_exists\"][0,:,1] == 1\n",
    "  o = {\"pae\":pae[mask,:][:,mask],\n",
    "       \"plddt\":plddt[mask],\n",
    "       \"sm_contacts\":sm_contacts[mask,:][:,mask],\n",
    "       \"xyz\":xyz[mask]}\n",
    "  return o\n",
    "\n",
    "def get_hash(x): return hashlib.sha1(x.encode()).hexdigest()\n",
    "alphabet_list = list(ascii_uppercase+ascii_lowercase)\n",
    "\n",
    "# jobname = \"test\" #@param {type:\"string\"}\n",
    "jobname = 'test'\n",
    "# jobname = re.sub(r'\\W+', '', jobname)[:50]\n",
    "\n",
    "sequence = \"VINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPGKSIGGDIFSNREGKLPGKSGRTWREADINYTSGFRNSDRILYSSDWLIYKTTDHYQTFTKIR\" #@param {type:\"string\"}\n",
    "sequence = re.sub(\"[^A-Z:]\", \"\", sequence.replace(\"/\",\":\").upper())\n",
    "sequence = re.sub(\":+\",\":\",sequence)\n",
    "sequence = re.sub(\"^[:]+\",\"\",sequence)\n",
    "sequence = re.sub(\"[:]+$\",\"\",sequence)\n",
    "copies = 1 #@param {type:\"integer\"}\n",
    "if copies == \"\" or copies <= 0: copies = 1\n",
    "sequence = \":\".join([sequence] * copies)\n",
    "num_recycles = 3 #@param [\"0\", \"1\", \"2\", \"3\", \"6\", \"12\", \"24\"] {type:\"raw\"}\n",
    "chain_linker = 25\n",
    "\n",
    "ID = jobname+\"_\"+get_hash(sequence)[:5]\n",
    "seqs = sequence.split(\":\")\n",
    "lengths = [len(s) for s in seqs]\n",
    "length = sum(lengths)\n",
    "print(\"length\",length)\n",
    "\n",
    "u_seqs = list(set(seqs))\n",
    "if len(seqs) == 1: mode = \"mono\"\n",
    "elif len(u_seqs) == 1: mode = \"homo\"\n",
    "else: mode = \"hetero\"\n",
    "\n",
    "if \"model\" not in dir():\n",
    "  import torch\n",
    "  model = torch.load(\"esmfold.model\")\n",
    "  model.eval().cuda().requires_grad_(False)\n",
    "\n",
    "# optimized for Tesla T4\n",
    "if length > 700:\n",
    "  model.set_chunk_size(64)\n",
    "else:\n",
    "  model.set_chunk_size(128)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "output = model.infer(sequence,\n",
    "                     num_recycles=num_recycles,\n",
    "                     chain_linker=\"X\"*chain_linker,\n",
    "                     residue_index_offset=512)\n",
    "\n",
    "pdb_str = model.output_to_pdb(output)[0]\n",
    "output = tree_map(lambda x: x.cpu().numpy(), output)\n",
    "ptm = output[\"ptm\"][0]\n",
    "plddt = output[\"plddt\"][0,...,1].mean()\n",
    "O = parse_output(output)\n",
    "print(f'ptm: {ptm:.3f} plddt: {plddt:.3f}')\n",
    "os.system(f\"mkdir -p {ID}\")\n",
    "prefix = f\"{ID}/ptm{ptm:.3f}_r{num_recycles}_default\"\n",
    "np.savetxt(f\"{prefix}.pae.txt\",O[\"pae\"],\"%.3f\")\n",
    "with open(f\"{prefix}.pdb\",\"w\") as out:\n",
    "  out.write(pdb_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title display mutation {run: \"auto\"}\n",
    "view = py3Dmol.view(width=400, height=300)\n",
    "view.addModelsAsFrames(pdb_str)\n",
    "\n",
    "\n",
    "mutation = \"V10N\" #@param {type:\"string\"}\n",
    "pos = mutation[1:-1]\n",
    "\n",
    "show_sidechains = False #@param {type:\"boolean\"}\n",
    "show_mainchains = False #@param {type:\"boolean\"}\n",
    "\n",
    "if show_sidechains:\n",
    "    BB = ['C','O','N']\n",
    "    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n",
    "                  {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
    "    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n",
    "                  {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
    "    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
    "                  {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
    "if show_mainchains:\n",
    "  BB = ['C','O','N','CA']\n",
    "  view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
    "\n",
    "i = 0\n",
    "for line in pdb_str.split(\"\\n\"):\n",
    "    split = line.split()\n",
    "    if len(split) == 0 or split[0] != \"ATOM\":\n",
    "        continue\n",
    "    if split[5] == pos:\n",
    "        color = \"yellow\"\n",
    "        view.addStyle({'model': -1, 'serial': i+1}, {\"cartoon\": {'color': color}})\n",
    "        view.addStyle({'model': -1, 'serial': i+1}, {\"stick\": {\"colorscheme\": \"yellowCarbon\"}})\n",
    "    else:\n",
    "        # print(split)\n",
    "        color = \"green\"\n",
    "        view.addStyle({'model': -1, 'serial': i+1}, {\"cartoon\": {'color': color}})\n",
    "    idx = int(split[1])\n",
    "\n",
    "\n",
    "    i += 1\n",
    "view.zoomTo()\n",
    "view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Protein Stability Assessment using Transformers",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
